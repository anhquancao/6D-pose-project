{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquancva/DenseFusion/lib/transformations.py:1912: UserWarning: failed to import module _transformations\n",
      "  warnings.warn('failed to import module %s' % name)\n"
     ]
    }
   ],
   "source": [
    "from lib.knn.__init__ import KNearestNeighbor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "from lib.network import PoseNet\n",
    "from lib.loss import Loss\n",
    "from lib.transformations import euler_matrix, quaternion_matrix, quaternion_from_matrix\n",
    "from datasets.linemod.dataset import PoseDataset as PoseDataset_linemod\n",
    "import yaml\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_objects = 5\n",
    "objlist = [2, 4, 5, 10, 11]\n",
    "num_points = 500\n",
    "iteration = 4\n",
    "refine_start = False\n",
    "bs = 1\n",
    "dataset_config_dir = 'datasets/linemod/dataset_config'\n",
    "output_result_dir = 'experiments/eval_result/linemod'\n",
    "knn = KNearestNeighbor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'trained_models/linemod/pose_model_8_0.011674933365175446.pth'\n",
    "estimator = PoseNet(num_points = num_points, num_obj = num_objects)\n",
    "estimator.to(device)\n",
    "estimator.load_state_dict(torch.load(model))\n",
    "estimator.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquancva/DenseFusion/datasets/linemod/dataset.py:66: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  self.meta[item] = yaml.load(meta_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object 2 buffer loaded\n",
      "Object 4 buffer loaded\n",
      "Object 5 buffer loaded\n",
      "Object 10 buffer loaded\n",
      "Object 11 buffer loaded\n"
     ]
    }
   ],
   "source": [
    "dataset_root = \"./datasets/linemod/Linemod_preprocessed\"\n",
    "\n",
    "\n",
    "testdataset = PoseDataset_linemod('eval', \n",
    "                              num_points, \n",
    "                              True, \n",
    "                              dataset_root, \n",
    "                              0.0, \n",
    "                              refine_start)\n",
    "testdataloader = torch.utils.data.DataLoader(testdataset, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquancva/.conda/envs/visual/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "sym_list = testdataset.get_sym_list()\n",
    "num_points_mesh = testdataset.get_num_points_mesh()\n",
    "criterion = Loss(num_points_mesh, sym_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.024750624233, 0.017249224865, 0.020140358597000002, 0.016462758847999998, 0.017588933422000002]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aquancva/.conda/envs/visual/lib/python3.6/site-packages/ipykernel_launcher.py:3: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "diameter = []\n",
    "meta_file = open('{0}/models_info.yml'.format(dataset_config_dir), 'r')\n",
    "meta = yaml.load(meta_file)\n",
    "for obj in objlist:\n",
    "    diameter.append(meta[obj]['diameter'] / 1000.0 * 0.1)\n",
    "print(diameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_count = [0 for i in range(num_objects)]\n",
    "num_count = [0 for i in range(num_objects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = open('{0}/eval_result_logs.txt'.format(output_result_dir), 'w')\n",
    "for i, data in enumerate(testdataloader, 0):\n",
    "    points, choose, img, target, model_points, idx = data\n",
    "    \n",
    "    if len(points.size()) == 2:\n",
    "        print('No.{0} NOT Pass! Lost detection!'.format(i))\n",
    "        fw.write('No.{0} NOT Pass! Lost detection!\\n'.format(i))\n",
    "        continue\n",
    "    points, choose, img, target, model_points, idx = Variable(points).cuda(), \\\n",
    "                                                     Variable(choose).cuda(), \\\n",
    "                                                     Variable(img).cuda(), \\\n",
    "                                                     Variable(target).cuda(), \\\n",
    "                                                     Variable(model_points).cuda(), \\\n",
    "                                                     Variable(idx).cuda()\n",
    "    pred_r, pred_t, pred_c, emb = estimator(img, points, choose, idx)\n",
    "    pred_r = pred_r / torch.norm(pred_r, dim=2).view(1, num_points, 1)\n",
    "    pred_c = pred_c.view(bs, num_points)\n",
    "    how_max, which_max = torch.max(pred_c, 1)\n",
    "    pred_t = pred_t.view(bs * num_points, 1, 3)\n",
    "\n",
    "    my_r = pred_r[0][which_max[0]].view(-1).cpu().data.numpy()\n",
    "    my_t = (points.view(bs * num_points, 1, 3) + pred_t)[which_max[0]].view(-1).cpu().data.numpy()\n",
    "    my_pred = np.append(my_r, my_t)\n",
    "    \n",
    "    # Here 'my_pred' is the final pose estimation result ('my_r': quaternion, 'my_t': translation)\n",
    "    \n",
    "    model_points = model_points[0].cpu().detach().numpy()\n",
    "    my_r = quaternion_matrix(my_r)[:3, :3]\n",
    "    pred = np.dot(model_points, my_r.T) + my_t\n",
    "    target = target[0].cpu().detach().numpy()\n",
    "    \n",
    "    if idx[0].item() in sym_list:\n",
    "        pred = torch.from_numpy(pred.astype(np.float32)).cuda().transpose(1, 0).contiguous()\n",
    "        target = torch.from_numpy(target.astype(np.float32)).cuda().transpose(1, 0).contiguous()\n",
    "        inds = knn(target.unsqueeze(0), pred.unsqueeze(0))\n",
    "        target = torch.index_select(target, 1, inds.view(-1) - 1)\n",
    "        dis = torch.mean(torch.norm((pred.transpose(1, 0) - target.transpose(1, 0)), dim=1), dim=0).item()\n",
    "    else:\n",
    "        dis = np.mean(np.linalg.norm(pred - target, axis=1))\n",
    "\n",
    "    if dis < diameter[idx[0].item()]:\n",
    "        success_count[idx[0].item()] += 1\n",
    "#         print('No.{0} Pass! Distance: {1}'.format(i, dis))\n",
    "        fw.write('No.{0} Pass! Distance: {1}\\n'.format(i, dis))\n",
    "    else:\n",
    "#         print('No.{0} NOT Pass! Distance: {1}'.format(i, dis))\n",
    "        fw.write('No.{0} NOT Pass! Distance: {1}\\n'.format(i, dis))\n",
    "    num_count[idx[0].item()] += 1\n",
    "    \n",
    "for i in range(num_objects):\n",
    "    print('Object {0} success rate: {1}'.format(objlist[i], float(success_count[i]) / num_count[i]))\n",
    "    fw.write('Object {0} success rate: {1}\\n'.format(objlist[i], float(success_count[i]) / num_count[i]))\n",
    "print('ALL success rate: {0}'.format(float(sum(success_count)) / sum(num_count)))\n",
    "fw.write('ALL success rate: {0}\\n'.format(float(sum(success_count)) / sum(num_count)))\n",
    "fw.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Object 2 success rate: 0.8516003879728419\n",
    "- Object 4 success rate: 0.7686274509803922\n",
    "- Object 5 success rate: 0.8868110236220472\n",
    "- Object 10 success rate: 0.9981185324553151\n",
    "- Object 11 success rate: 0.9527027027027027\n",
    "- ALL success rate: 0.8925667828106852\n",
    "\n",
    "\n",
    "- 2: \"Bench Vise\"\n",
    "- 4: \"camera\"\n",
    "- 5: \"can\"\n",
    "- 6: \"cat\"\n",
    "- 10: \"eggbox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recvis",
   "language": "python",
   "name": "recvis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
